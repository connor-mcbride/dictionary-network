\documentclass[12pt,oneside]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz}
\usetikzlibrary{cd}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{booktabs} 
\allowdisplaybreaks
\graphicspath{ {./images/} }

\newenvironment{nouppercase}{%
    \let\uppercase\relax%
    \renewcommand{\uppercasenonmath}[1]{}}{}

\titleformat{\section}[block]{\filcenter\Large\bfseries}{\thesection}{1em}{}

% THEOREMS -------------------------------------------------------

\numberwithin{equation}{section}
\numberwithin{figure}{section}

\theoremstyle{plain}
\newtheorem{thm}[equation]{Theorem}
\newtheorem*{FundClaim*}{Fundamental Claim}
\newtheorem{lemma}[equation]{Lemma}
\newtheorem{cor}[equation]{Corollary}
\newtheorem*{prop}{Proposition}
\newtheorem{example}[equation]{Example}
\newtheorem{prob}{Problem}

\theoremstyle{definition}
\newtheorem{definition}[equation]{Definition}
\newtheorem{question}[equation]{Question}
\newtheorem{remark}[equation]{Remark}


% MATH -----------------------------------------------------------
\newcommand{\Q}{\ensuremath \mathbb{Q}}
\newcommand{\R}{\ensuremath \mathbb{R}}
\newcommand{\C}{\ensuremath \mathbb{C}}
\newcommand{\Z}{\ensuremath \mathbb{Z}}
\newcommand{\N}{\ensuremath \mathbb{N}}
\newcommand{\M}{\ensuremath \mathbb{M}}
\newcommand{\F}{\ensuremath \mathbb{F}}
\newcommand{\Ord}{\text{Ord}}

\newcommand{\lxor}{\underline{\lor}}
\newcommand{\lnor}{\overline{\lor}}
\newcommand{\lnand}{\overline{\land}}
\newcommand{\dom}[1]{\text{dom}(#1)}
\newcommand{\ran}[1]{\text{ran}(#1)}
\newcommand{\rref}[1]{#1^\text{ref}}
\newcommand{\rsym}[1]{#1^\text{sym}}
\newcommand{\rtran}[1]{#1^\text{tran}}
\newcommand{\rirref}[1]{#1^\text{irref}}
\newcommand{\rasym}[1]{#1^\text{asym}}
\newcommand{\rantisym}[1]{#1^\text{antisym}}
\newcommand{\rintran}[1]{#1^\text{intran}}
\newcommand{\ldef}{\text{iff}_\text{def}}
\newcommand{\lub}[1]{\text{lub}_#1}
\newcommand{\glb}[1]{\text{glb}_#1}
\newcommand{\canon}[1]{#1_{\text{canon}}}
\newcommand{\pset}[2][]{\mathcal{P}^{#1}(#2)}
\newcommand{\fset}[2][]{\mathcal{F}^{#1}(#2)}
\newcommand{\restrict}[2]{#1\mid_{#2}}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\Log}{\text{Log}}
\newcommand{\Arg}{\text{Arg}}
\newcommand{\Arcsin}{\text{Arcsin}}
\newcommand{\Res}{\text{Res}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\newcommand{\deriv}[3][]{\frac{\mathrm{d}^{#1}#2}{\mathrm{d}{#3}^{#1}}}
\renewcommand{\epsilon}{\varepsilon}
\providecommand{\Tau}{\mathrm{T}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\title{Math 525 Project Report}
\author{Connor McBride}
\date{Webster's Unabridged English Dictionary}

\begin{document}

\maketitle

\section{Sourcing the Data}

The source of the data comes from the 2009 English \textit{Webster's Unabridged Dictionary} made available as an eBook by Project Gutenberg \cite{webster_unabridged_dict_2009}. 
I used a repository by Matthew Reagan \cite{reagan_webstersenglishdictionary} to parse the eBook file and turn the dictionary into JSON format. Downloading the JSON file, I was then able to write a Python script \cite{mcbride_dictionary_network} that would convert the data into a NetworkX \cite{hagberg2008exploring} directed graph for analysis.

To generate the network, we first went through dictionary and created a node for each word that has a definition in the dictionary. We then iterated through each definition and created a directed edge from the word being defined to each other word appearing in its definition if that word in the definition is also defined. The weights for the edges are the number of times that the word appears in the definition (See Table \ref{fig:edge_example} for example).
\begin{figure}[!h]
\begin{center}
    \includegraphics[scale=0.5]{edge_example.png}
\end{center}
    \caption{An example of edge creation. Five weighted edges are created in total, Two weighted edges are created from ``disgraduate'' to ``to'' and ``degrade'' with edge weight corresponding to the number of times it appears in the definition. ``Obs'' and ``Tyndale'' are not defined and therefore no edges are created to these words.}
    \label{fig:edge_example}
\end{figure}

\section{Network Analysis}

\subsection{Basic Structure}
The basic structure of the dictionary network is given in Table \ref{tab:basic_properties}. Certain definitions contained examples sentences using the word in them, meaning that we had 24,872 self-loops in the network. For certain calculations such as the k-cores, we had to remove the self-loops as the algorithm wouldn't work with them.

\begin{table}[!hbtp]
\centering
\begin{tabular}{c c c}
\textbf{Number of Nodes} & \textbf{Number of Edges} & \textbf{Edge Type} \\ 
\hline
102,217 & 1,704,423 & Directed; weighted
\end{tabular}
\caption{Basic Properties of dictionary network.}
\label{tab:basic_properties}
\end{table}

\subsection{Centralities}
Computing the centrality measures for the dictionary network presented some computational challenges due to the large number of nodes and edges. To deal with these issues, we used a combination of sparse array operations in SciPy \cite{virtanen2020scipy} with the adjacency matrix and the igraph package \cite{csardi2006igraph} which implements the centrality algorithms in C.

The eigenvector and Katz centalities could be interpreted as ``process'' words that are useful when defining words. Betweenness centrality could be interpreted as the most common vocabulary words used. In-degree can be interpreted as the most utilized words in definitions while out-degree can be seen as the longest/most verbose definitions. In particular, ``run'' and ``set'' have the most definitions in the dictionary, which could help explain their high centrality measures. The top 4 words for each centrality can be found in Figure \ref{fig:centrality_measures}.

\begin{figure}[ht]
\centering

\begin{subfigure}[t]{0.30\linewidth}
\centering
\caption{Eigenvector}
\begin{tabular}{lc}
\toprule
Word & Centrality \\
\midrule
set  & 0.0004389 \\
run  & 0.0004184 \\
take & 0.0004115 \\
turn & 0.0003981 \\
\bottomrule
\end{tabular}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.30\linewidth}
\centering
\caption{Betweenness}
\begin{tabular}{lc}
\toprule
Word & Centrality \\
\midrule
a   & 0.02968116 \\
of  & 0.02508660 \\
to  & 0.02088582 \\
see & 0.01153032 \\
\bottomrule
\end{tabular}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.30\linewidth}
\centering
\caption{Katz}
\begin{tabular}{lc}
\toprule
Word & Centrality \\
\midrule
set  & 1.7745e--05 \\
run  & 1.6958e--05 \\
turn & 1.6621e--05 \\
take & 1.6354e--05 \\
\bottomrule
\end{tabular}
\end{subfigure}

\vspace{1em}

\begin{subfigure}[t]{0.45\linewidth}
\centering
\caption{In-degree}
\begin{tabular}{lc}
\toprule
Word & Degree \\
\midrule
a   & 59{,}405 \\
of  & 57{,}317 \\
the & 52{,}744 \\
or  & 45{,}817 \\
\bottomrule
\end{tabular}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\linewidth}
\centering
\caption{Out-degree}
\begin{tabular}{lc}
\toprule
Word & Degree \\
\midrule
set  & 750 \\
run  & 674 \\
turn & 645 \\
take & 617 \\
\bottomrule
\end{tabular}
\end{subfigure}

\caption{Centrality and degree statistics for dictionary network.}
\label{fig:centrality_measures}
\end{figure}

\subsection{Connected Components}
Examining the connected components, we see there are 573 weak components and 73,039 strong components. As demonstrated in Figure \ref{fig:component_distributions}, the majority of components are quite small with just one large component.
The largest strong component contains 27,637 words, while the largest weak component contains 101,642 nodes. This means that 99.4\% of words are weakly reachable to each other. The small strong components are usually made up of different words that are part of the same word family, e.g. ``lawyerlike'' and ``lawyerly.''

\begin{figure}[ht]
\centering

\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{weak_components.png}
    \label{fig:first}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{strong_components.png}
    \label{fig:second}
\end{subfigure}

\caption{Plotted distributions of sizes of weak and strong components.}
\label{fig:component_distributions}
\end{figure}

\subsection{Community Detection}
We employed the Leiden Algorithm written in C++ by the leidenalg package \cite{traag2019leiden} for directed modularity groups. There are 621 detected communities with the largest one of size 33,285. 
The largest communities contained very common words while the smallest communities were often made up of word families. The most interesting communities that contained words making up specific scientific disciplines or other niche subjects. Three of which are shown in Figure \ref{fig:community_examples}.

We then build a meta graph, which takes uses detected communities as nodes and inter-community edges as edges. Examining the active meta-graph of the Leiden communities (nodes with inter-community edges), we can see that four communities are very influential, with edges connecting with 80-96\% of the other communities.

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[scale=0.5]{community_sizes.png}
\end{center}
    \caption{Sizes of detected communities.}
    \label{fig:community_sizes}
\end{figure}

\begin{figure}[!htbp]
\centering

\begin{subfigure}[t]{0.31\linewidth}
\centering
\caption{Seismic Community}
\begin{tabular}{l}
\toprule
Earthdin \\
Anaseismic \\
Terremote \\
Earthquake \\
Earthshock \\
Earthquave \\
\bottomrule
\end{tabular}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.31\linewidth}
\centering
\caption{Pre-historic Community}
\begin{tabular}{l}
\toprule
Paleolithic \\
Neanderthal \\
Supraciliary \\
Dolichocephaly \\
Superciliary \\
\bottomrule
\end{tabular}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.31\linewidth}
\centering
\caption{Infernal Community}
\begin{tabular}{l}
\toprule
Satanical \\
Erebus \\
Devilish \\
Infernal \\
Hellish \\
Diabolical \\
Fiendlike \\
\bottomrule
\end{tabular}
\end{subfigure}

\caption{Three semantic communities computed along with appropriate nicknames.}
\label{fig:community_examples}
\end{figure}

\subsection{K-Core Analysis}
We next examine the $k$-cores of the dictionary network. The largest $k$-core was of degree 121 with 521 words. The largest $k$-cores were surprisingly large, as the cores of size $110+$ contained thousands of words. Sampling example words from the different sizes of $k$-cores reveals that the cores of size $1-50$ contain the more obscure words, while words in cores of size $120+$ contain those that make up the semantic backbone of the dictionary (Table \ref{tab:kcore_samples}).

\begin{table}[!hbtp]
\centering
\begin{tabular}{c c}
\textbf{Core Size} & \textbf{Examples} \\ 
\hline
1-50 & anopheles, sarong, turcoman, corrugator \\
120+ & I, most, some rest
\end{tabular}
\caption{Example words for different $k$-core sizes.}
\label{tab:kcore_samples}
\end{table}

\section{Real World Properties}

\subsection{Power Law}
The degree histogram resembles the power law pattern. The fitted value power law exponent $\alpha \approx 1.452$, meaning that it does not decay quite as fast as most real-world networks. This was done with $\texttt{k\_min} = 1$ and a calculated standard error of $\sim 0.0013$.
\begin{figure}[!b]
\begin{center}
    \includegraphics[scale=0.372]{power_law.png}
\end{center}
    \caption{Power law curve fit to degrees distribution.}
    \label{fig:power_law}
\end{figure}

\subsection{Network Measures}
The network's degree assortativity of $\sim -0.21$ means that the network is moderately disassortative, resembling more of a technological/biological network rather than a social network. $\sim 0.042$ is very low reciprocity for a real-world network, but for a dictionary makes sense. Dictionaries try to avoid immediately circular definitions, but are not always perfect.
Finally, the global clustering coefficient is $\sim 0.215$, and there exists evidence of strong local clustering. This would make sense as synonyms and words dealing with the same subject would tend to cluster. This value is typical for social and biological networks. Overall, these networks are consistent with numbers found in real-world networks.

\begin{table}[!hbtp]
\centering
\begin{tabular}{c c}
\textbf{Measure} & \textbf{Value} \\ 
\hline
Degree Assortativity Coefficient & $\sim$ -0.210191 \\
Reciprocity & $\sim$ 0.042064 \\
Global Clustering Coefficient & $\sim$ 0.215269
\end{tabular}
\caption{Different network measures for the dictionary network.}
\label{tab:network_measures}
\end{table}

\subsection{Giant Component and Potential Models}
As previously discussed, there exists a giant component in the weak sense, as 99.4\% of the words belong to the giant weak component. There is not a giant component in the strong sense as only 27.04\% of nodes belong the largest strong component. The existence of the weak giant component helps to argue that the network contains features of real world networks.

It is difficult to find a model that would incorporate all of the features characteristic of the dictionary network. One potential choice of model is a directed configuration model. This would allow us to specify that exact in-/out-degree sequences that we would expect and thus similar degree distributions and degree-based statistics.
This model lacks, however, the desired clustering, reciprocity, and community structures that we have demonstrated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography below
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier % Keep the figures from being put after the bibliography
\newpage
\bibliography{refs}
\bibliographystyle{alpha}

\end{document}
